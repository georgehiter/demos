"""
æ ¸å¿ƒLCELç»„ä»¶ - å®ç°Runnableæ¥å£çš„åŒæ­¥ç‰ˆæœ¬

åŒ…å«ç†è®ºæå–å™¨ã€è¡¨æ ¼æå–å™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½æ”¯æŒåŒæ­¥è°ƒç”¨ã€‚
"""

import re
from typing import Dict, List, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

from langchain_core.runnables import Runnable, RunnableLambda
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


@dataclass
class TableData:
    """è¡¨æ ¼æ•°æ®ç»“æ„"""

    title: str
    headers: List[str]
    rows: List[List[str]]
    start_line: int = 0
    end_line: int = 0
    interpretation: str = ""


class TheoryExtractor(Runnable):
    """
    ç†è®ºæ¡†æ¶æå–å™¨ - å®ç°Runnableæ¥å£

    ä½¿ç”¨Mock LLMæå–ç†è®ºæ¡†æ¶ï¼Œæ”¯æŒåŒæ­¥è°ƒç”¨ã€‚
    """

    def __init__(self, llm_manager=None, enable_llm_summary: bool = True):
        """åˆå§‹åŒ–ç†è®ºæå–å™¨"""
        self.llm_manager = llm_manager
        self.enable_llm_summary = enable_llm_summary

    def invoke(self, inputs: Dict[str, Any], config: Any = None) -> Dict[str, Any]:
        """
        åŒæ­¥è°ƒç”¨æ¥å£ - ç¬¦åˆLCEL Runnableæ ‡å‡†

        Args:
            inputs: è¾“å…¥å‚æ•°å­—å…¸ï¼Œå¿…é¡»åŒ…å«contenté”®
            config: é…ç½®å‚æ•°ï¼ˆLCELæ ‡å‡†å‚æ•°ï¼‰

        Returns:
            Dict[str, Any]: æå–çš„ç†è®ºæ¡†æ¶ï¼ŒåŒ…å«ç»“æ„åŒ–ä¿¡æ¯
        """
        content = inputs.get("content", "")
        theory_data = self.extract_theory(content)

        # è¿”å›ç»Ÿä¸€ç»“æ„çš„å­—å…¸
        return {
            "type": "theory_framework",
            "content": theory_data,
            "status": "success",
            "summary": "ç†è®ºæ¡†æ¶æå–å®Œæˆ",
            "metadata": {
                "llm_summary_used": "LLMæ€»ç»“" in theory_data,
                "has_content": len(theory_data) > 0,
            },
        }

    def extract_theory(self, content: str) -> Dict[str, List[str]]:
        """
        æå–ç†è®ºæ¡†æ¶çš„æ ¸å¿ƒé€»è¾‘

        Args:
            content: ç†è®ºæ–‡æœ¬å†…å®¹

        Returns:
            Dict[str, List[str]]: æå–çš„ç†è®ºæ¡†æ¶
        """
        # è·å–å‰20è¡Œå†…å®¹ç”¨äºLLMåˆ†æ
        lines = content.split("\n")
        first_20_lines = lines[:20]

        print(f"ğŸ“ [Theory] ä½¿ç”¨å‰20è¡Œå†…å®¹è¿›è¡Œåˆ†æï¼Œå…±{len(first_20_lines)}è¡Œ")

        # å°è¯•ä½¿ç”¨LLMè¿›è¡Œæ€»ç»“
        if self.enable_llm_summary and self.llm_manager:
            print(f"ğŸ¤– [Theory] å°è¯•ä½¿ç”¨LLMè¿›è¡Œå†…å®¹æ€»ç»“...")
            summary = self._summarize_with_llm(content)
            if summary:
                print(f"âœ… [Theory] LLMæ€»ç»“å®Œæˆ")
                return {"LLMæ€»ç»“": summary}
            else:
                print(f"â„¹ï¸ [Theory] ä½¿ç”¨åŸæœ‰é€»è¾‘ï¼Œæœªè¿›è¡ŒLLMæ€»ç»“")

        # å¦‚æœæ²¡æœ‰LLMæ€»ç»“ï¼Œè¿”å›ç©ºç»“æœ
        return {}

    def _create_summary_prompt(self, content: str) -> str:
        """
        åˆ›å»ºæ€»ç»“æç¤ºè¯

        Args:
            content: éœ€è¦æ€»ç»“çš„å†…å®¹

        Returns:
            str: æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç†è®ºåˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç†è®ºå†…å®¹è¿›è¡Œç®€æ´çš„æ€»ç»“ï¼Œæå–æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®æ¦‚å¿µï¼š

{content}

è¯·æä¾›ç®€æ´æ˜äº†çš„æ€»ç»“ï¼Œé‡ç‚¹çªå‡ºç†è®ºçš„æ ¸å¿ƒå†…å®¹å’Œä¸»è¦è§‚ç‚¹ã€‚"""

    def _summarize_with_llm(self, content: str) -> str:
        """
        ä½¿ç”¨LLMè¿›è¡Œå†…å®¹æ€»ç»“

        Args:
            content: éœ€è¦æ€»ç»“çš„å†…å®¹

        Returns:
            str: æ€»ç»“ç»“æœï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not self.llm_manager or not self.enable_llm_summary:
            return ""

        try:
            return self.llm_manager.invoke(self._create_summary_prompt(content)) or ""
        except:
            return ""


class TableExtractor(Runnable):
    """
    æç®€è¡¨æ ¼æå–å™¨ - æ”¯æŒè¡¨æ ¼æå–å’Œå¹¶è¡ŒLLMæ ¼å¼åŒ–

    ä½¿ç”¨ThreadPoolExecutorå®ç°å¹¶è¡Œå¤„ç†ï¼Œä»£ç ç®€æ´æ˜“ç»´æŠ¤ã€‚
    """

    def __init__(self, llm_manager=None):
        """åˆå§‹åŒ–è¡¨æ ¼æå–å™¨"""
        self.llm_manager = llm_manager

    def invoke(self, inputs: Dict[str, Any], config: Any = None) -> Dict[str, Any]:
        """
        åŒæ­¥è°ƒç”¨æ¥å£ - ç¬¦åˆLCEL Runnableæ ‡å‡†
        """
        if "content" not in inputs:
            raise ValueError("è¾“å…¥å¿…é¡»åŒ…å«contenté”®")

        content = inputs["content"]
        tables = self._extract_tables(content)

        if self.llm_manager:
            print(f"ğŸ¤– [Table] è¯†åˆ«åˆ°{len(tables)}ä¸ªè¡¨æ ¼ï¼Œå¼€å§‹å¹¶è¡ŒLLMæ ¼å¼åŒ–...")
            # å¹¶è¡ŒLLMæ ¼å¼åŒ–
            with ThreadPoolExecutor() as executor:
                futures = [
                    executor.submit(self._format_table, table) for table in tables
                ]
                results = [f.result() for f in futures]
            print(f"âœ… [Table] æ‰€æœ‰è¡¨æ ¼LLMæ ¼å¼åŒ–å®Œæˆ")
            return {
                "content": results,
                "status": "success",
                "metadata": {"table_count": len(results)},
            }
        else:
            return {
                "content": tables,
                "status": "success",
                "metadata": {"table_count": len(tables)},
            }

    def _extract_tables(self, content: str) -> List[List[List[str]]]:
        """æå–æ‰€æœ‰è¡¨æ ¼å†…å®¹"""
        lines = content.split("\n")
        tables = []
        i = 0

        while i < len(lines):
            if self._is_table_line(lines[i]):
                table_data = self._extract_single_table(lines, i)
                if table_data:
                    tables.append(table_data["content"])
                    i = table_data["end_line"] + 1
                else:
                    i += 1
            else:
                i += 1

        return tables

    def _is_table_line(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼è¡Œ"""
        return "|" in line.strip() and line.strip().count("|") >= 2

    def _extract_single_table(self, lines: List[str], start: int) -> dict:
        """æå–å•ä¸ªè¡¨æ ¼"""
        table_content = []
        i = start

        while i < len(lines) and self._is_table_line(lines[i]):
            line = lines[i].strip()
            # è·³è¿‡åˆ†éš”ç¬¦è¡Œ
            if not re.match(r"^\|[\s\-:|]+\|$", line):
                cells = [cell.strip() for cell in line.split("|")[1:-1]]
                if cells:
                    table_content.append(cells)
            i += 1

        if table_content:
            return {"content": table_content, "end_line": i - 1}
        return None

    def _format_table(self, table_content: List[List[str]]) -> dict:
        """æ ¼å¼åŒ–å•ä¸ªè¡¨æ ¼"""
        try:
            prompt = self._create_formatting_prompt(table_content)
            formatted_table = self.llm_manager.invoke(prompt)
            if formatted_table:
                return {
                    "åŸå§‹è¡¨æ ¼": table_content,
                    "LLMæ ¼å¼åŒ–è¡¨æ ¼": formatted_table,
                }
            else:
                return {"åŸå§‹è¡¨æ ¼": table_content}
        except Exception as e:
            print(f"âš ï¸ [Table] è¡¨æ ¼æ ¼å¼åŒ–å¤±è´¥: {e}")
            return {"åŸå§‹è¡¨æ ¼": table_content}

    def _create_formatting_prompt(self, table_content: List[List[str]]) -> str:
        """åˆ›å»ºè¡¨æ ¼æ ¼å¼åŒ–æç¤ºè¯"""
        table_str = "\n".join(["| " + " | ".join(row) + " |" for row in table_content])

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹è¡¨æ ¼è¿›è¡Œç»“æ„æ•´ç†å’Œæ ¼å¼åŒ–ï¼š

{table_str}

è¯·é‡æ–°ç»„ç»‡è¡¨æ ¼ç»“æ„ï¼Œä¼˜åŒ–åˆ—çš„é¡ºåºå’Œå†…å®¹ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°æ˜“è¯»ã€‚ä¿æŒæ•°æ®çš„å®Œæ•´æ€§ï¼Œè¾“å‡ºæ ¼å¼åŒ–çš„è¡¨æ ¼å†…å®¹ã€‚è¯·ä½¿ç”¨Markdownè¡¨æ ¼æ ¼å¼ã€‚"""


class ReportGenerator(Runnable):
    """
    æŠ¥å‘Šç”Ÿæˆå™¨ - å®ç°Runnableæ¥å£

    åŸºäºç†è®ºå’Œè¡¨æ ¼æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨å®Œå…¨LCELåŒ–çš„ç®¡é“ã€‚
    """

    def __init__(self, llm_manager):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.llm_manager = llm_manager

        # åˆ›å»º PromptTemplate æç¤ºè¯æ¨¡æ¿
        self.prompt_template = PromptTemplate.from_template(
            """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿åŸºäºç†è®ºå’Œæ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚

è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è®ºæ–‡åˆ†ææŠ¥å‘Šï¼š

## ç†è®ºæ¡†æ¶
{theory_content}

## è¡¨æ ¼æ•°æ®
{tables_content}

è¯·ç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å†…å®¹è¯¦å®çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»è¦ç†è®ºè§‚ç‚¹
2. æ•°æ®åˆ†æå’Œå‘ç°

å›ç­”ä»…éœ€è¦ä»¥ä¸Šä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸éœ€è¦æ€»ç»“ã€‚"""
        )

        # åˆ›å»ºå®Œæ•´çš„ LCEL ç®¡é“
        self.chain = (
            RunnablePassthrough.assign(
                theory_content=lambda x: x["theory"].get("content", "æš‚æ— ç†è®ºæ¡†æ¶æ•°æ®"),
                tables_content=lambda x: x["tables"].get("content", "æš‚æ— è¡¨æ ¼æ•°æ®"),
            )
            | self.prompt_template
            | self.llm_manager
            | StrOutputParser()
        )

    def invoke(self, inputs: Dict[str, Any], config: Any = None) -> Dict[str, Any]:
        """
        åŒæ­¥è°ƒç”¨æ¥å£ - ç¬¦åˆLCEL Runnableæ ‡å‡†

        Args:
            inputs: è¾“å…¥å‚æ•°å­—å…¸ï¼ŒåŒ…å«theoryå’Œtablesé”®
            config: é…ç½®å‚æ•°ï¼ˆLCELæ ‡å‡†å‚æ•°ï¼‰

        Returns:
            Dict[str, Any]: ç”Ÿæˆçš„æŠ¥å‘Šï¼ŒåŒ…å«ç»“æ„åŒ–ä¿¡æ¯
        """
        # æ£€æŸ¥è¾“å…¥å†…å®¹
        theory_framework = inputs.get("theory", {})
        tables = inputs.get("tables", {})

        # æ£€æŸ¥æ˜¯å¦éƒ½ä¸ºç©º
        if not theory_framework.get("content") and not tables.get("content"):
            return {
                "type": "report",
                "content": "âš ï¸ è­¦å‘Šï¼šç†è®ºæ¡†æ¶å’Œè¡¨æ ¼æ•°æ®éƒ½ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæœ‰æ„ä¹‰çš„åˆ†ææŠ¥å‘Šã€‚",
                "status": "warning",
                "summary": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š",
                "metadata": {},
            }

        # å°è¯•ä½¿ç”¨LLMè¿›è¡ŒæŠ¥å‘Šç”Ÿæˆ
        print(f"ğŸ¤– [Report] å°è¯•ä½¿ç”¨LLMè¿›è¡ŒæŠ¥å‘Šç”Ÿæˆ...")
        response = self.chain.invoke(inputs)
        print(f"âœ… [Report] LLMæŠ¥å‘Šç”Ÿæˆå®Œæˆ")

        # è¿”å›ç»Ÿä¸€ç»“æ„çš„å­—å…¸
        return {
            "type": "report",
            "content": response,
            "status": "success",
            "summary": "æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "metadata": {
                "word_count": len(response.split()),
                "char_count": len(response),
                "llm_report_used": True,
            },
        }
